{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93018468-156f-4571-8290-05dc9fd419ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import fasttext\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7547676-51ae-46cd-9005-776baba2ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wmt14_dataset = load_dataset('wmt14', 'de-en', split='validation')\n",
    "cnn_dailymail_dataset = load_dataset('cnn_dailymail', '2.0.0', split='validation')\n",
    "gsm8k_dataset = load_dataset('openai/gsm8k', 'main', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd692bb6-1d90-43b8-8ee8-1352f520264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(labelled_dataset):\n",
    "    def format_for_fasttext(row):\n",
    "        return f\"__label__{row['Chosen_Model']} {row['Input_Text']}\"\n",
    "    \n",
    "    formatted_data = labelled_dataset.apply(format_for_fasttext, axis=1)\n",
    "    formatted_data.to_csv('fasttext_train.txt', index=False, header=False)\n",
    "\n",
    "    fasttext_classifier = fasttext.train_supervised('fasttext_train.txt', epoch=100)\n",
    "\n",
    "    # rint(f\"Done with training fasttext classifier!\")\n",
    "    return fasttext_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c034bd5-4b2f-4843-9c19-8ce9363725f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, model_outputs, task, idx):\n",
    "    input_text, label = model_outputs[task][idx][\"input\"], model_outputs[task][idx][\"label\"]\n",
    "    predicted_label, confidence_score = classifier.predict(input_text)\n",
    "    large_model_output = model_outputs[task][idx][\"13b\"]\n",
    "\n",
    "    model = \"\"\n",
    "    if \"7b\" in predicted_label[0]: output = model_outputs[task][idx][\"7b\"]; model = \"7b\"\n",
    "    elif \"tiny\" in predicted_label[0]: output = model_outputs[task][idx][\"tiny\"]; model = \"tiny\"\n",
    "    elif \"13b\" in predicted_label[0]: output = model_outputs[task][idx][\"13b\"]; model = \"tiny\"\n",
    "\n",
    "    normalized_bleu_diff = 0\n",
    "    normalized_rouge_diff = 0\n",
    "    normalized_acc_diff = 0\n",
    "    \n",
    "    if task == \"wmt14\":\n",
    "        bleu_score_classifier = sentence_bleu([label.split()], output[0].split())\n",
    "        bleu_score_13b = sentence_bleu([label.split()], large_model_output[0].split())\n",
    "        bleu_diff = bleu_score_classifier - bleu_score_13b\n",
    "        normalized_bleu_diff = bleu_diff / max(bleu_score_classifier, bleu_score_13b, 1e-6)\n",
    "\n",
    "    if task == \"cnn_dailymail\":\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores_classifier = scorer.score(label, output[0])\n",
    "        scores_13b = scorer.score(label, large_model_output[0])\n",
    "        rouge1_diff = scores_classifier['rouge1'].fmeasure - scores_13b['rouge1'].fmeasure\n",
    "        rouge2_diff = scores_classifier['rouge2'].fmeasure - scores_13b['rouge2'].fmeasure\n",
    "        rougeL_diff = scores_classifier['rougeL'].fmeasure - scores_13b['rougeL'].fmeasure\n",
    "        avg_rouge_diff = (rouge1_diff + rouge2_diff + rougeL_diff) / 3\n",
    "        normalized_rouge_diff = avg_rouge_diff / max(scores_classifier['rougeL'].fmeasure, scores_13b['rougeL'].fmeasure, 1e-6) \n",
    "        \n",
    "    if task == \"gsm8k\":\n",
    "        def extract_final_number(raw_outputs):\n",
    "            matches = re.findall(r'\\b\\d+\\b', raw_outputs)\n",
    "            if matches:\n",
    "                return matches[-1]  \n",
    "            return None\n",
    "\n",
    "        answer = extract_final_number(output[0])\n",
    "        large_model_answer = extract_final_number(large_model_output[0])\n",
    "        if answer == label:\n",
    "            classifier_correct = 1\n",
    "        else:\n",
    "            classifier_correct = 0\n",
    "        \n",
    "        if large_model_answer == label:\n",
    "            large_model_correct = 1\n",
    "        else:\n",
    "            large_model_correct = 0\n",
    "        \n",
    "        acc_diff = classifier_correct - large_model_correct\n",
    "        normalized_acc_diff = acc_diff / max(classifier_correct, large_model_correct, 1e-6) \n",
    "\n",
    "    \n",
    "    emissions = carbon_emissions[model][task]\n",
    "    if emissions == 1.774e-4:\n",
    "        print(True)\n",
    "    \n",
    "    return (normalized_bleu_diff + normalized_rouge_diff + normalized_acc_diff) / 3, emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc017967-9a6b-4f05-a900-009006c7908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_dataset(similarity_scores, mid):\n",
    "    input_texts = []\n",
    "    chosen_models = []\n",
    "    threshold = mid\n",
    "\n",
    "    for idx, pair in enumerate(similarity_scores['wmt14']['tiny_13b']):\n",
    "        # For WMT14 dataset\n",
    "        input_text = wmt14_dataset[idx]['translation']['de']\n",
    "        similarity_tiny_13b = similarity_scores['wmt14']['tiny_13b'][idx]\n",
    "        similarity_7b_13b = similarity_scores['wmt14']['7b_13b'][idx]\n",
    "    \n",
    "        if similarity_tiny_13b >= threshold: chosen_model = 'tiny'\n",
    "        elif similarity_7b_13b >= threshold: chosen_model = '7b'\n",
    "        else: chosen_model = '13b'  \n",
    "    \n",
    "        input_texts.append(input_text)\n",
    "        chosen_models.append(chosen_model)\n",
    "\n",
    "    for idx, pair in enumerate(similarity_scores['cnn_dailymail']['tiny_13b']):\n",
    "        # For CNN/DailyMail dataset\n",
    "        input_text = cnn_dailymail_dataset[idx]['article']\n",
    "        similarity_tiny_13b = similarity_scores['cnn_dailymail']['tiny_13b'][idx]\n",
    "        similarity_7b_13b = similarity_scores['cnn_dailymail']['7b_13b'][idx]\n",
    "    \n",
    "        if similarity_tiny_13b >= threshold: chosen_model = 'tiny'\n",
    "        elif similarity_7b_13b >= threshold: chosen_model = '7b'\n",
    "        else: chosen_model = '13b'  \n",
    "    \n",
    "        input_texts.append(input_text)\n",
    "        chosen_models.append(chosen_model)\n",
    "\n",
    "    for idx, pair in enumerate(similarity_scores['gsm8k']['tiny_13b']):\n",
    "        # For GSM8K dataset\n",
    "        input_text = gsm8k_dataset[idx]['question']\n",
    "        similarity_tiny_13b = similarity_scores['gsm8k']['tiny_13b'][idx]\n",
    "        similarity_7b_13b = similarity_scores['gsm8k']['7b_13b'][idx]\n",
    "    \n",
    "        if similarity_tiny_13b >= threshold: chosen_model = 'tiny'\n",
    "        elif similarity_7b_13b >= threshold: chosen_model = '7b'\n",
    "        else: chosen_model = '13b'  \n",
    "    \n",
    "        input_texts.append(input_text)\n",
    "        chosen_models.append(chosen_model)\n",
    "        \n",
    "    labelled_dataset = {'Input_Text': input_texts, 'Chosen_Model': chosen_models}\n",
    "    df = pd.DataFrame(labelled_dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6290c7d-f9bd-473d-91e8-f22afc2e8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_model_carbon = 3.569e-2 + 6.511e-2 + 1.067e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4f507-dcc4-490a-8407-650adb5e30cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_carbon = 9.278e-3 + 4.141e-2 + 1.616e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda854a6-9608-4e45-82aa-b407e449ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(middle_model_carbon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43f8e5-7f26-4ce4-93ab-c5afce23988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_emissions = {\n",
    "    \"tiny\": {\"wmt14\": 9.278e-6, \"cnn_dailymail\": 4.141e-5, \"gsm8k\": 3.616e-4},\n",
    "    \"7b\": {\"wmt14\": 3.569e-5, \"cnn_dailymail\": 6.511e-5, \"gsm8k\": 1.067e-4},\n",
    "    \"13b\": {\"wmt14\": 5.059e-5, \"cnn_dailymail\": 1.430e-4, \"gsm8k\": 1.774e-4}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57955e4e-9039-40e1-8599-68d275c84574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(similarity_scores, model_outputs):\n",
    "    low = 0\n",
    "    high = 1\n",
    "    epsilon = 0.01\n",
    "    best_threshold = None\n",
    "    best_difference = float('inf')\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) / 2\n",
    "        print(mid)\n",
    "        \n",
    "        new_dataset = create_new_dataset(similarity_scores, mid)\n",
    "        classifier = train_classifier(new_dataset)\n",
    "        \n",
    "        total_difference = 0\n",
    "        total_carbon_emissions = 0\n",
    "\n",
    "        tasks = [\"wmt14\", \"cnn_dailymail\", \"gsm8k\"]\n",
    "        for task in tasks:\n",
    "            for idx in range(1000):\n",
    "                difference, emissions = evaluate_classifier(classifier, model_outputs, task, idx)\n",
    "                total_difference += difference\n",
    "                total_carbon_emissions += emissions\n",
    "\n",
    "        print(total_carbon_emissions)\n",
    "        print(middle_model_carbon)\n",
    "        if abs(total_difference) < best_difference and total_carbon_emissions < middle_model_carbon:\n",
    "            best_difference = abs(total_difference)\n",
    "            best_threshold = mid\n",
    "            low = mid + epsilon\n",
    "        else:\n",
    "            high = mid - epsilon\n",
    "\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cccecfc-63bc-466a-b2f9-1e4dd3ddb31d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"similarity_scores.pkl\", \"rb\") as f:\n",
    "    similarity_scores = pickle.load(f)\n",
    "with open(\"combined_outputs.pkl\", \"rb\") as f:\n",
    "    model_outputs = pickle.load(f)\n",
    "\n",
    "optimal_threshold = find_optimal_threshold(similarity_scores, model_outputs)\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b4af2-65a7-479c-bd24-13464042ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991f0b3-ea23-420f-a99a-ac4a7d98499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset = create_new_dataset(similarity_scores, threshold)\n",
    "fasttext_classifier = train_classifier(labelled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3828ec-73b8-4b8b-b43c-0163890aeb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687dd1d3-fe5c-430c-99ec-61f40c585092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bleu_scores_sum = 0\n",
    "num_7b = 0\n",
    "num_tiny = 0\n",
    "num_13b = 0\n",
    "\n",
    "for idx in range(1000):\n",
    "    input_text, label = model_outputs[\"wmt14\"][idx][\"input\"], model_outputs[\"wmt14\"][idx][\"label\"]\n",
    "    predicted_label, confidence_score = fasttext_classifier.predict(input_text)\n",
    "    if \"7b\" in predicted_label[0]: \n",
    "        num_7b += 1\n",
    "        output = model_outputs[\"wmt14\"][idx][\"7b\"]\n",
    "    elif \"tiny\" in predicted_label[0]: \n",
    "        num_tiny += 1\n",
    "        output = model_outputs[\"wmt14\"][idx][\"tiny\"]\n",
    "    elif \"13b\" in predicted_label[0]: \n",
    "        num_13b += 1\n",
    "        output = model_outputs[\"wmt14\"][idx][\"13b\"]\n",
    "            \n",
    "    bleu_scores_sum += sentence_bleu([label.split()], output[0].split())        \n",
    "\n",
    "avg_bleu_wmt14 = bleu_scores_sum / 1000\n",
    "print(f\"Avg Bleu WMT14: {avg_bleu_wmt14}\")\n",
    "print(num_7b)\n",
    "print(num_tiny)\n",
    "print(num_13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c74e7b-f636-4059-83ef-752885a628b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores_sum_7b = 0\n",
    "\n",
    "for idx in range(1000):\n",
    "    input_text = model_outputs[\"wmt14\"][idx][\"input\"]\n",
    "    label = model_outputs[\"wmt14\"][idx][\"label\"]\n",
    "    candidate = model_outputs[\"wmt14\"][idx][\"13b\"][0]  \n",
    "\n",
    "    bleu_score = sentence_bleu([label.split()], candidate.split())\n",
    "    bleu_scores_sum_7b += bleu_score\n",
    "\n",
    "avg_bleu_wmt14_7b = bleu_scores_sum_7b / 1000\n",
    "print(f\"Avg BLEU 7b WMT14: {avg_bleu_wmt14_7b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2ff45-a2ec-4566-ba57-b482c5780a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = create_new_dataset(similarity_scores, 87)\n",
    "classifier = train_classifier(new_dataset)\n",
    "classifier.save_model('fasttext_classifier87.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd5110-83f6-4f8d-889e-6cc59ea77b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4775dd-bbb3-4e46-ad69-fb5e6f8814c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17882c57-7718-4f68-ab95-f6665c25738e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5003270-2043-4eeb-a780-ba73de6ac4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf0745-16c9-4954-a1dd-43f683bf8067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432fc86-7ae9-4beb-bc6c-8c336cd5c211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742f7e8-1e9e-4cca-9e84-864c0c3dd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold_linear(similarity_scores, model_outputs):\n",
    "    low = 0\n",
    "    high = 1\n",
    "    epsilon = 0.01\n",
    "    best_threshold = None\n",
    "    best_difference = float('inf')\n",
    "    \n",
    "    threshold = low\n",
    "    while threshold <= high:\n",
    "        new_dataset = create_new_dataset(similarity_scores, threshold)\n",
    "        classifier = train_classifier(new_dataset)\n",
    "        \n",
    "        total_difference = 0\n",
    "        total_carbon_emissions = 0\n",
    "\n",
    "        tasks = [\"wmt14\", \"cnn_dailymail\", \"gsm8k\"]\n",
    "        for task in tasks:\n",
    "            for idx in range(1000):\n",
    "                difference, emissions = evaluate_classifier(classifier, model_outputs, task, idx)\n",
    "                total_difference += difference\n",
    "                total_carbon_emissions += emissions\n",
    "\n",
    "        if abs(total_difference) < best_difference and total_carbon_emissions < middle_model_carbon:\n",
    "            best_difference = abs(total_difference)\n",
    "            best_threshold = threshold\n",
    "\n",
    "        threshold += epsilon\n",
    "            \n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea34ac4-988a-4de8-9033-e9412ab0c632",
   "metadata": {},
   "source": [
    "Pre-Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b939d-a5d2-4734-9812-f6127243273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78632ad2-d0a3-4924-85c7-03085f1f937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "llama7b_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llama7b = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2b653-764e-4137-b53a-de28c6a3fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "tinyllama_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tinyllama = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e8379-3077-4c8b-b33c-5a7d03d4e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama13_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                    bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164de4b3-f8cf-44d3-b801-f0f85f2fec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "llama13b_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llama13b = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', quantization_config=llama13_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6867da-2a0b-4d97-a914-d92eecc08411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "wmt14_dataset = load_dataset('wmt14', 'de-en', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119ec41-9101-4bab-9eb9-928dacb08956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    outputs = []\n",
    "    \n",
    "    input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "    input_prompt = \"Translate the sentence from German to English: \\n\\n\" + input_text + \"\\n\\n Write the translation here: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'])\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    answer_prefix = \"Write the translation here: \"\n",
    "    if answer_prefix in output_text:\n",
    "        cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "\n",
    "    first_sentence = cleaned_output.split('.')[0] + '.' if '.' in cleaned_output else cleaned_output\n",
    "    outputs.append(first_sentence)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d766f-592f-49d5-8040-14cd21488376",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "outputs_7b = []\n",
    "outputs_tiny = []\n",
    "outputs_13b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162a318-9050-4ff7-ad50-08eb9d4e7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_wmt14(model, tokenizer, start, array, model_name):\n",
    "    for current_idx in range(start, 1000):\n",
    "        input_text = wmt14_dataset[current_idx]['translation']['de']\n",
    "        output = generate_output(model, tokenizer, input_text, current_idx)\n",
    "        array.append(output)\n",
    "        \n",
    "        print(f\"{model_name} | CURRENT IDX: {current_idx} | Length: {len(array)}\")\n",
    "        if (model_name == \"Llama7b\"): \n",
    "            with open('THRESHOLDING_input_output_pairs_wmt14_7b', 'wb') as f: pickle.dump(array, f)\n",
    "        if (model_name == \"TinyLlama\"): \n",
    "            with open('THRESHOLDING_input_output_pairs_wmt14_tiny', 'wb') as f: pickle.dump(array, f)\n",
    "        if (model_name == \"Llama13b\"): \n",
    "            with open('THRESHOLDING_input_output_pairs_wmt14_13b', 'wb') as f: pickle.dump(array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a40b7-6505-431b-8aac-c44921188b36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inference_wmt14(llama7b, llama7b_tokenizer, 0, outputs_7b, \"Llama7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945fac2-4c9b-44ae-a8fb-9d16498bca0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inference_wmt14(tinyllama, tinyllama_tokenizer, 0, outputs_tiny, \"TinyLlama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017a9a0-9fc4-4192-b87b-0ba8c339c167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference_wmt14(llama13b, llama13b_tokenizer, 971, outputs_13b, \"Llama13b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078cea3-495e-4ee0-aef3-7f8de8da5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"THRESHOLDING_input_output_pairs_wmt14_7b\", \"rb\") as f: outputs_7b = pickle.load(f)\n",
    "print(len(outputs_7b))\n",
    "with open(\"THRESHOLDING_input_output_pairs_wmt14_tiny\", \"rb\") as f: outputs_tiny = pickle.load(f)\n",
    "print(len(outputs_tiny))\n",
    "with open(\"THRESHOLDING_input_output_pairs_wmt14_13b\", \"rb\") as f: outputs_13b = pickle.load(f)\n",
    "print(len(outputs_13b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b761f6b-4988-4b63-b3ff-79c577119d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "cnn_dailymail_dataset = load_dataset('abisee/cnn_dailymail', '2.0.0', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63aa896-86e8-4902-8cc0-20365b0b3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    outputs = []\n",
    "    \n",
    "    input_text = cnn_dailymail_dataset[current_idx]['article'] \n",
    "    input_prompt = \"Summarize the following text in under 50 words: \\n\\n\" + input_text + \"\\n\\n Write the summary here: \"\n",
    "    \n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'], max_new_tokens=100)\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    summary_prefix = \"Write the summary here: \"\n",
    "    if summary_prefix in output_text:\n",
    "        cleaned_output = output_text.split(summary_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "\n",
    "    outputs.append(cleaned_output)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaffa22-7c71-4b10-b5fd-00336dc32299",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "outputs_7b = []\n",
    "outputs_tiny = []\n",
    "outputs_13b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b3402-6068-41c4-b9c2-b4ffe2be85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_cnn_dailymail(model, tokenizer, start, array, model_name):\n",
    "    for current_idx in range(start, 1000):\n",
    "        input_text = cnn_dailymail_dataset[current_idx]['article'] \n",
    "        output = generate_output(model, tokenizer, input_text, current_idx)\n",
    "        array.append(output)\n",
    "        \n",
    "        print(f\"{model_name} | CURRENT IDX: {current_idx} | Length: {len(array)}\")\n",
    "        if (model_name == \"Llama7b\"): \n",
    "            with open('THRESHOLDING_input_output_pairs_cnn_dailymail_7b', 'wb') as f: pickle.dump(array, f)\n",
    "        if (model_name == \"TinyLlama\"): \n",
    "            with open('THRESHOLDING_input_output_pairs_cnn_dailymail_tiny', 'wb') as f: pickle.dump(array, f)\n",
    "        if (model_name == \"Llama13b\"): \n",
    "            with open('THRESHOLDING_input_output_pairs_cnn_dailymail_13b', 'wb') as f: pickle.dump(array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a7e37-8358-40a2-bd23-c5ed02edaf04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inference_cnn_dailymail(llama7b, llama7b_tokenizer, 0, outputs_7b, \"Llama7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489403e-5a2e-47c8-bff7-4ce2428fab4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inference_cnn_dailymail(tinyllama, tinyllama_tokenizer, 0, outputs_tiny, \"TinyLlama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda7a0a0-599a-4e34-9b10-4ad064cd5dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference_cnn_dailymail(llama13b, llama13b_tokenizer, 860, outputs_13b, \"Llama13b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108b8ac-4133-4ef4-a931-d229f79bcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"THRESHOLDING_input_output_pairs_cnn_dailymail_7b\", \"rb\") as f: outputs_7b = pickle.load(f)\n",
    "print(len(outputs_7b))\n",
    "with open(\"THRESHOLDING_input_output_pairs_cnn_dailymail_tiny\", \"rb\") as f: outputs_tiny = pickle.load(f)\n",
    "print(len(outputs_tiny))\n",
    "with open(\"THRESHOLDING_input_output_pairs_cnn_dailymail_13b\", \"rb\") as f: outputs_13b = pickle.load(f)\n",
    "print(len(outputs_13b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a6112-ac3f-4580-b27f-c52aaf3085db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "gsm8k_dataset = load_dataset('openai/gsm8k', 'main', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e5824-6d2c-43f4-a56b-c86ea274834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(model, tokenizer, dataset, current_idx):\n",
    "    outputs = []\n",
    "    \n",
    "    input_text = gsm8k_dataset[current_idx]['question'] \n",
    "    input_prompt = \"Answer the following math question: \\n\\n\" + input_text + \"\\n\\n Lets think step by step: \"\n",
    "\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    output = model.generate(inputs['input_ids'])\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    answer_prefix = \"Lets think step by step: \"\n",
    "    if answer_prefix in output_text:\n",
    "        cleaned_output = output_text.split(answer_prefix)[-1].strip()\n",
    "    else:\n",
    "        cleaned_output = output_text.strip()\n",
    "\n",
    "    outputs.append(cleaned_output)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2d382-b189-476d-ac33-b6dccc546b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "outputs_7b = []\n",
    "outputs_tiny = []\n",
    "outputs_13b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcd3d2-b256-4c68-b45e-560a784bd6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_gsm8k(model, tokenizer, start, array, model_name):\n",
    "    for current_idx in range(start, 1000):\n",
    "        input_text = gsm8k_dataset[current_idx]['question'] \n",
    "        output = generate_output(model, tokenizer, input_text, current_idx)\n",
    "        array.append(output)\n",
    "        \n",
    "        print(f\"{model_name} | CURRENT IDX: {current_idx} | Length: {len(array)}\")\n",
    "        if (model_name == \"Llama7b\"): \n",
    "            with open('THRESHOLDING_input_output_pairs_gsm8k_7b', 'wb') as f: pickle.dump(array, f)\n",
    "        if (model_name == \"TinyLlama\"): \n",
    "            with open('THRESHOLDING_input_output_pairs_gsm8k_tiny', 'wb') as f: pickle.dump(array, f)\n",
    "        if (model_name == \"Llama13b\"): \n",
    "            with open('THRESHOLDING_input_output_pairs_gsm8k_13b', 'wb') as f: pickle.dump(array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead5bf1-ae5f-4008-b81b-8dc88a10d8a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inference_gsm8k(llama7b, llama7b_tokenizer, 0, outputs_7b, \"Llama7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a32be-d5a8-4713-aae2-d7b1ac250e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inference_gsm8k(tinyllama, tinyllama_tokenizer, 0, outputs_tiny, \"TinyLlama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b5087-67ec-4ea6-aec4-6636ed2a9731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inference_gsm8k(llama13b, llama13b_tokenizer, 997, outputs_13b, \"Llama13b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd545f8-c4dc-4989-8bab-21575b6927a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"THRESHOLDING_input_output_pairs_wmt14_7b\", \"rb\") as f: outputs_wmt14_7b = pickle.load(f)\n",
    "print(f\"WMT14 7b: {len(outputs_wmt14_7b)}\")\n",
    "\n",
    "with open(\"THRESHOLDING_input_output_pairs_wmt14_tiny\", \"rb\") as f: outputs_wmt14_tiny = pickle.load(f)\n",
    "print(f\"WMT14 Tiny: {len(outputs_wmt14_tiny)}\")\n",
    "\n",
    "with open(\"THRESHOLDING_input_output_pairs_wmt14_13b\", \"rb\") as f: outputs_wmt14_13b = pickle.load(f)\n",
    "print(f\"WMT14 13b: {len(outputs_wmt14_13b)}\")\n",
    "\n",
    "# Load outputs for CNN/DailyMail\n",
    "with open(\"THRESHOLDING_input_output_pairs_cnn_dailymail_7b\", \"rb\") as f: outputs_cnn_dailymail_7b = pickle.load(f)\n",
    "print(f\"CNN/DailyMail 7b: {len(outputs_cnn_dailymail_7b)}\")\n",
    "\n",
    "with open(\"THRESHOLDING_input_output_pairs_cnn_dailymail_tiny\", \"rb\") as f: outputs_cnn_dailymail_tiny = pickle.load(f)\n",
    "print(f\"CNN/DailyMail Tiny: {len(outputs_cnn_dailymail_tiny)}\")\n",
    "\n",
    "with open(\"THRESHOLDING_input_output_pairs_cnn_dailymail_13b\", \"rb\") as f: outputs_cnn_dailymail_13b = pickle.load(f)\n",
    "print(f\"CNN/DailyMail 13b: {len(outputs_cnn_dailymail_13b)}\")\n",
    "\n",
    "# Load outputs for GSM8K\n",
    "with open(\"THRESHOLDING_input_output_pairs_gsm8k_7b\", \"rb\") as f: outputs_gsm8k_7b = pickle.load(f)\n",
    "print(f\"GSM8K 7b: {len(outputs_gsm8k_7b)}\")\n",
    "\n",
    "with open(\"THRESHOLDING_input_output_pairs_gsm8k_tiny\", \"rb\") as f: outputs_gsm8k_tiny = pickle.load(f)\n",
    "print(f\"GSM8K Tiny: {len(outputs_gsm8k_tiny)}\")\n",
    "\n",
    "with open(\"THRESHOLDING_input_output_pairs_gsm8k_13b\", \"rb\") as f: outputs_gsm8k_13b = pickle.load(f)\n",
    "print(f\"GSM8K 13b: {len(outputs_gsm8k_13b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d4282-0a59-4686-8180-c6a6f5ee2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"THRESHOLDING_input_output_pairs_gsm8k_7b\", \"rb\") as f: outputs_7b = pickle.load(f)\n",
    "print(len(outputs_7b))\n",
    "with open(\"THRESHOLDING_input_output_pairs_gsm8k_tiny\", \"rb\") as f: outputs_tiny = pickle.load(f)\n",
    "print(len(outputs_tiny))\n",
    "with open(\"THRESHOLDING_input_output_pairs_gsm8k_13b\", \"rb\") as f: outputs_13b = pickle.load(f)\n",
    "print(len(outputs_13b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d559c3-aefa-4492-8f02-2afc6642d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_dataset(outputs_7b, outputs_tiny, outputs_13b, dataset, input_key, label_key):\n",
    "    combined = []\n",
    "    for idx in range(len(outputs_7b)):\n",
    "        if input_key == 'translation':  # Special case for WMT14\n",
    "            sample = {\n",
    "                'input': dataset[idx]['translation']['de'],\n",
    "                'label': dataset[idx]['translation']['en'],\n",
    "                '7b': outputs_7b[idx],\n",
    "                'tiny': outputs_tiny[idx],\n",
    "                '13b': outputs_13b[idx]\n",
    "            }\n",
    "        else:\n",
    "            sample = {\n",
    "                'input': dataset[idx][input_key],\n",
    "                'label': dataset[idx][label_key],\n",
    "                '7b': outputs_7b[idx],\n",
    "                'tiny': outputs_tiny[idx],\n",
    "                '13b': outputs_13b[idx]\n",
    "            }\n",
    "        combined.append(sample)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49540582-7bf8-4b94-bae4-c49dd042bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_wmt14 = create_combined_dataset(outputs_wmt14_7b, outputs_wmt14_tiny, outputs_wmt14_13b, wmt14_dataset, 'translation', 'translation')\n",
    "combined_cnn_dailymail = create_combined_dataset(outputs_cnn_dailymail_7b, outputs_cnn_dailymail_tiny, outputs_cnn_dailymail_13b, cnn_dailymail_dataset, 'article', 'highlights')\n",
    "combined_gsm8k = create_combined_dataset(outputs_gsm8k_7b, outputs_gsm8k_tiny, outputs_gsm8k_13b, gsm8k_dataset, 'question', 'answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08492f25-16e3-4604-9dcc-06de25d55a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_outputs = {\n",
    "    'wmt14': combined_wmt14,\n",
    "    'cnn_dailymail': combined_cnn_dailymail,\n",
    "    'gsm8k': combined_gsm8k\n",
    "}\n",
    "\n",
    "with open(\"combined_outputs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(combined_outputs, f)\n",
    "\n",
    "print(\"Combined outputs have been saved to 'combined_outputs.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ed39f-c078-401f-aac4-762d54dc1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_outputs['wmt14'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e6e17-33e0-406c-ae3a-a31a982298b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
